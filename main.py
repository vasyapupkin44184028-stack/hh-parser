import requests
import pandas as pd
import time
import re
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
import os
from urllib.parse import urlparse, quote_plus
from datetime import datetime
from fake_useragent import UserAgent
from playwright.sync_api import sync_playwright
import random

class HHApiClient:
    def __init__(self):
        self.base_url = "https://api.hh.ru"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
        })
        self.regions_cache = None
    
    def load_regions(self):
        if self.regions_cache is not None:
            return self.regions_cache
            
        try:
            print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤...")
            response = self.session.get(f"{self.base_url}/areas", timeout=10)
            if response.status_code == 200:
                regions_data = response.json()
                regions_dict = {}
                
                def parse_areas(areas):
                    for area in areas:
                        area_name = area['name'].lower()
                        regions_dict[area_name] = area['id']
                        if area['id'] == '1':
                            regions_dict['–º—Å–∫'] = '1'
                            regions_dict['moscow'] = '1'
                        elif area['id'] == '2':
                            regions_dict['—Å–ø–±'] = '2'
                            regions_dict['–ø–∏—Ç–µ—Ä'] = '2'
                        elif area['id'] == '3':
                            regions_dict['–µ–∫–±'] = '3'
                        elif area['id'] == '4':
                            regions_dict['–Ω—Å–∫'] = '4'
                        elif area['id'] == '66':
                            regions_dict['–Ω–Ω'] = '66'
                        
                        if 'areas' in area and area['areas']:
                            parse_areas(area['areas'])
                
                parse_areas(regions_data)
                self.regions_cache = regions_dict
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ä–µ–≥–∏–æ–Ω–æ–≤: {len(regions_dict)}")
                return regions_dict
            else:
                print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤")
                return {}
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ä–µ–≥–∏–æ–Ω–æ–≤: {e}")
            return {}
    
    def search_vacancies(self, text, area=113, page=0, per_page=100):
        url = f"{self.base_url}/vacancies"
        params = {
            'text': text,
            'area': area,
            'page': page,
            'per_page': per_page,
        }
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ API: {e}")
            return None

class CompanyWebsiteFinder:
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.update_headers()
        self.website_cache = {}
        
        self.known_websites = {
            '—Å–æ–≤–∫–æ–º–±–∞–Ω–∫': 'sovcombank.ru', 'neoflex': 'neoflex.ru', 'aston': 'aston.ru',
            '—Ç-–±–∞–Ω–∫': 'tbank.ru', 'ibs': 'ibs.ru', '–∞–ª–∞–±—É–≥–∞': 'alabuga.ru', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ': 'tinkoff.ru',
            '—Å–±–µ—Ä': 'sber.ru', '—è–Ω–¥–µ–∫—Å': 'yandex.ru', 'mail.ru': 'mail.ru', 'vkontakte': 'vk.com',
            'ozon': 'ozon.ru', 'wildberries': 'wildberries.ru', 'avito': 'avito.ru', 'dns': 'dns-shop.ru',
            'mvideo': 'mvideo.ru', '—Å–∏—Ç–∏–ª–∏–Ω–∫': 'citilink.ru', '–≥–∞–∑–ø—Ä–æ–º': 'gazprom.ru', '–ª—É–∫–æ–π–ª': 'lukoil.ru',
            '—Ä–∂–¥': 'rzd.ru', '—Ä–æ—Å—Ç–µ—Ö': 'rostec.ru', '—Ä–æ—Å–∞—Ç–æ–º': 'rosatom.ru', '–º–µ–≥–∞—Ñ–æ–Ω': 'megafon.ru',
            '–º—Ç—Å': 'mts.ru', '–±–∏–ª–∞–π–Ω': 'beeline.ru', 'tele2': 'tele2.ru', '—Ç–æ–ø': 'top-academy.ru',
            'idf': 'idfeurasia.com', 'eurasia': 'idfeurasia.com',
            '–∞–ª—å—Ñ–∞': 'alfabank.ru', '–≤—Ç–±': 'vtb.ru', '–æ—Ç–∫—Ä—ã—Ç–∏–µ': 'open.ru', '—Ä–æ—Å–±–∞–Ω–∫': 'rosbank.ru',
            'qiwi': 'qiwi.com', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è': 'kaspersky.ru', '–∫–∞—Å–ø–µ—Ä—Å–∫–∏–π': 'kaspersky.ru',
            '1—Å': '1c.ru', '–±–∏—Ç—Ä–∏–∫—Å': 'bitrix24.ru', '–∞–≥–≤–∏—Ä': 'agvir.ru', '–º–µ–¥–∏–∞–ª–æ–≥–∏—è': 'mlg.ru',
            '–∫–æ–Ω—Ç—É—Ä': 'kontur.ru', '—Å–∫–∞–π–µ–Ω–≥': 'skyeng.ru', '–Ω–µ—Ç–æ–ª–æ–≥–∏—è': 'netology.ru',
            '–≥–µ–¥–µ–æ–Ω': 'gideon.ru', '—Å–∏–±–∏–Ω—Ç–µ–∫': 'sibintek.ru', '—Ñ–∞–∫—Ç–æ—Ä': 'factor.ru',
            '—Ç–µ–º–∞': 'tema.ru', '—Ç–µ–ª–µ–∫–æ–º': 'tema.ru'
        }
    
    def update_headers(self):
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        })
    
    def find_company_website(self, company_name):
        if not company_name or company_name == "–ù–µ —É–∫–∞–∑–∞–Ω–æ":
            return None
        
        if company_name in self.website_cache:
            return self.website_cache[company_name]
        
        start_time = time.time()
        print(f"üîç –ü–æ–∏—Å–∫ —Å–∞–π—Ç–∞ –¥–ª—è: {company_name}")
        
        known_site = self.check_known_websites(company_name)
        if known_site:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã: {known_site}")
            self.website_cache[company_name] = known_site
            return known_site
        
        generated_site = self.fast_generate_website_url(company_name)
        if generated_site:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é: {generated_site}")
            self.website_cache[company_name] = generated_site
            return generated_site
        
        playwright_site = self.playwright_search_ultra_fast(company_name)
        if playwright_site:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ Playwright: {playwright_site}")
            self.website_cache[company_name] = playwright_site
            return playwright_site
        
        print(f"‚ùå –°–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è: {company_name} (–ø–æ–∏—Å–∫ –∑–∞–Ω—è–ª {time.time()-start_time:.2f}—Å)")
        self.website_cache[company_name] = None
        return None
    
    def check_known_websites(self, company_name):
        clean_name = company_name.lower().strip()
        for known_company, domain in self.known_websites.items():
            if known_company in clean_name:
                url = f"https://{domain}"
                if self.ultra_fast_site_check(url):
                    return url
        return None
    
    def fast_generate_website_url(self, company_name):
        if not company_name:
            return None
        
        clean_name = re.sub(r'[\(\)\[\]\{\}]', '', company_name)
        clean_name = re.sub(r'[^\w\s]', ' ', clean_name).strip()
        if len(clean_name) < 2:
            return None
        
        name_variants = set()
        
        words = clean_name.split()
        if len(words) > 3:
            main_name = ' '.join(words[:2])
            name_variants.add(main_name.lower().replace(' ', ''))
            name_variants.add(main_name.lower().replace(' ', '-'))
        
        name_variants.add(clean_name.lower().replace(' ', ''))
        name_variants.add(clean_name.lower().replace(' ', '-'))
        
        translit_name = self.transliterate_cyrillic(clean_name)
        if translit_name:
            name_variants.add(translit_name.replace(' ', ''))
            name_variants.add(translit_name.replace(' ', '-'))
        
        domains = ['.ru', '.com', '.org', '.net']
        
        checked = 0
        for name in list(name_variants):
            for domain in domains:
                if checked >= 6:
                    break
                    
                url = f"https://{name}{domain}"
                if self.ultra_fast_site_check(url) and self.is_valid_company_site(url):
                    return url
                
                url_www = f"https://www.{name}{domain}"
                if self.ultra_fast_site_check(url_www) and self.is_valid_company_site(url_www):
                    return url_www
                
                checked += 1
        
        return None
    
    def transliterate_cyrillic(self, text):
        brand_exceptions = {
            '–∞–≤–∏—Ç–æ': 'avito', '—è–Ω–¥–µ–∫—Å': 'yandex', '—Å–±–µ—Ä': 'sber', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ': 'tinkoff',
            '–º–µ–≥–∞—Ñ–æ–Ω': 'megafon', '–º—Ç—Å': 'mts', '–±–∏–ª–∞–π–Ω': 'beeline', '—Ç–µ–ª–µ2': 'tele2',
            '–æ–∑–æ–Ω': 'ozon', '–≤–∫': 'vk', '–º–∞–∏–ª': 'mail', '—Ç–æ–ø': 'top', '–∞–∫–∞–¥–µ–º–∏—è': 'academy',
            'eurasia': 'eurasia', 'idf': 'idf', '–∞–ª—å—Ñ–∞': 'alfa', '–≤—Ç–±': 'vtb',
            'qiwi': 'qiwi', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è': 'kaspersky', '–∫–∞—Å–ø–µ—Ä—Å–∫–∏–π': 'kaspersky',
            '–±–∏—Ç—Ä–∏–∫—Å': 'bitrix', '–∞–≥–≤–∏—Ä': 'agvir', '–º–µ–¥–∏–∞–ª–æ–≥–∏—è': 'mlg',
            '–∫–æ–Ω—Ç—É—Ä': 'kontur', '—Å–∫–∞–π–µ–Ω–≥': 'skyeng', '–Ω–µ—Ç–æ–ª–æ–≥–∏—è': 'netology',
            '–≥–µ–¥–µ–æ–Ω': 'gideon', '—Å–∏–±–∏–Ω—Ç–µ–∫': 'sibintek', '—Ñ–∞–∫—Ç–æ—Ä': 'factor',
            '—Ç–µ–º–∞': 'tema', '—Ç–µ–ª–µ–∫–æ–º': 'telecom', '—Å–¥—ç–∫': 'cdek', '–ø–æ—á—Ç–∞': 'pochta',
            '–∞–ª–≥–æ—Ä–∏—Ç–º–∏–∫–∞': 'algorithmika', '–º–æ–Ω–æ–ª–∏—Ç': 'monolit'
        }
        
        text_lower = text.lower()
        for cyrillic, latin in brand_exceptions.items():
            if cyrillic in text_lower:
                return latin
        
        translit_dict = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch', '—ä': '',
            '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
        }
        
        result = []
        for char in text.lower():
            if char in translit_dict:
                result.append(translit_dict[char])
            elif char in 'abcdefghijklmnopqrstuvwxyz0123456789 -_':
                result.append(char)
            elif char == ' ':
                result.append(' ')
        
        return ''.join(result)
    
    def ultra_fast_site_check(self, url):
        try:
            response = requests.head(url, timeout=0.5, allow_redirects=True)
            return response.status_code == 200
        except:
            return False
    
    def playwright_search_ultra_fast(self, company_name):
        start_time = time.time()
        
        strategies = [
            self._playwright_strategy_stealth,
            self._playwright_strategy_humanized,
            self._playwright_strategy_fast_headless
        ]
        
        for strategy in strategies:
            if time.time() - start_time > 2:
                break
                
            try:
                result = strategy(company_name)
                if result:
                    return result
            except:
                continue
        
        return None
    
    def _playwright_strategy_stealth(self, company_name):
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,  # –ò–ó–ú–ï–ù–ï–ù–û: –±—ã–ª–æ False, —Å—Ç–∞–ª–æ True
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                    '--disable-ipc-flooding-protection',
                    '--disable-hang-monitor',
                    '--disable-popup-blocking',
                    '--disable-prompt-on-repost',
                    '--disable-back-forward-cache',
                    '--disable-component-extensions-with-background-pages',
                    '--disable-default-apps',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-translate',
                    '--disable-web-security',
                    '--allow-running-insecure-content',
                    '--no-first-run',
                    '--no-default-browser-check',
                    '--disable-client-side-phishing-detection',
                    '--disable-cookie-encryption',
                    '--disable-domain-reliability',
                    '--disable-print-preview',
                    '--disable-speech-api',
                    '--disable-sync',
                    '--disable-webaudio',
                    '--disable-webgl',
                    '--disable-webrtc',
                    '--force-color-profile=srgb',
                    '--metrics-recording-only',
                    '--mute-audio',
                    '--use-mock-keychain',
                    '--hide-scrollbars',
                    '--ignore-certificate-errors',
                    '--ignore-ssl-errors',
                    '--ignore-certificate-errors-spki-list',
                    '--log-level=3',
                    '--silent'
                ]
            )
            
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            
            viewports = [
                {'width': 1920, 'height': 1080},
                {'width': 1366, 'height': 768},
                {'width': 1536, 'height': 864},
                {'width': 1280, 'height': 720}
            ]
            
            config = {
                'user_agent': random.choice(user_agents),
                'viewport': random.choice(viewports),
                'locale': 'ru-RU',
                'timezone_id': 'Europe/Moscow',
                'geolocation': {'latitude': 55.7558, 'longitude': 37.6173},
                'permissions': ['geolocation']
            }
            
            context = browser.new_context(
                viewport=config['viewport'],
                user_agent=config['user_agent'],
                locale=config['locale'],
                timezone_id=config['timezone_id'],
                geolocation=config['geolocation'],
                permissions=config['permissions']
            )
            
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['ru-RU', 'ru', 'en-US', 'en']});
                Object.defineProperty(navigator, 'hardwareConcurrency', {get: () => 8});
                Object.defineProperty(navigator, 'deviceMemory', {get: () => 8});
                Object.defineProperty(screen, 'width', {get: () => 1920});
                Object.defineProperty(screen, 'height', {get: () => 1080});
                Object.defineProperty(screen, 'colorDepth', {get: () => 24});
                Object.defineProperty(Notification, 'permission', {get: () => 'default'});
                window.chrome = {runtime: {}};
                const originalQuery = window.navigator.permissions.query;
                window.navigator.permissions.query = (parameters) => (
                    parameters.name === 'notifications' ?
                        Promise.resolve({ state: Notification.permission }) :
                        originalQuery(parameters)
                );
            """)
            
            page = context.new_page()
            
            page.set_extra_http_headers({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            })
            
            try:
                clean_company = re.sub(r'[\(\)\[\]\{\}]', '', company_name)
                clean_company = re.sub(r'–ò–ü\s+\w+\s+\w+', '', clean_company).strip()
                
                search_query = f"{clean_company} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
                search_url = f"https://yandex.ru/search/?text={quote_plus(search_query)}&lr=213"
                
                page.goto(search_url, wait_until='domcontentloaded', timeout=2000)
                
                page.wait_for_timeout(100)
                
                if page.query_selector('form[action*="checkcaptcha"], .captcha, .CheckboxCaptcha'):
                    browser.close()
                    return None
                
                found_urls = []
                
                organic_selectors = [
                    'a.organic__greenurl',
                    '.serp-item a[href*="http"]',
                    '.organic a[href*="http"]',
                    '[data-cid] a[href*="http"]',
                    '.Path-Item a[href*="http"]',
                    '.organic__path a',
                    '[data-log-node*="organic"] a',
                    '.Organic a[href*="http"]'
                ]
                
                for selector in organic_selectors:
                    links = page.query_selector_all(selector)
                    for link in links[:5]:
                        try:
                            href = link.get_attribute('href')
                            text = link.text_content().strip() if link.text_content() else ""
                            
                            if href:
                                real_url = self.extract_real_url(href)
                                if real_url and self.is_valid_company_site_strict(real_url, company_name):
                                    if self.is_relevant_link(text, company_name):
                                        found_urls.append(real_url)
                        except:
                            continue
                
                if not found_urls:
                    all_links = page.query_selector_all('a[href*="http"]')
                    for link in all_links[:10]:
                        try:
                            href = link.get_attribute('href')
                            text = link.text_content().strip() if link.text_content() else ""
                            
                            if (href and 
                                not href.startswith('https://yandex.ru') and
                                not href.startswith('https://google.ru')):
                                
                                company_words = company_name.lower().split()
                                text_lower = text.lower()
                                
                                relevant = any(word in text_lower for word in company_words if len(word) > 2)
                                
                                if relevant:
                                    real_url = self.extract_real_url(href)
                                    if real_url and self.is_valid_company_site_strict(real_url, company_name):
                                        found_urls.append(real_url)
                        except:
                            continue
                
                unique_urls = list(set(found_urls))
                
                if unique_urls:
                    best_url = self.choose_best_url(unique_urls, company_name)
                    browser.close()
                    return best_url
                
                browser.close()
                return None
                
            except Exception as e:
                browser.close()
                return None
    
    def _playwright_strategy_humanized(self, company_name):
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,  # –ò–ó–ú–ï–ù–ï–ù–û: –±—ã–ª–æ False, —Å—Ç–∞–ª–æ True
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-background-timer-throttling'
                ]
            )
            
            context = browser.new_context(
                viewport={'width': 1366, 'height': 768},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='ru-RU'
            )
            
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            """)
            
            page = context.new_page()
            
            try:
                clean_company = re.sub(r'[\(\)\[\]\{\}]', '', company_name)
                clean_company = re.sub(r'–ò–ü\s+\w+\s+\w+', '', clean_company).strip()
                
                search_query = f"{clean_company} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
                search_url = f"https://yandex.ru/search/?text={quote_plus(search_query)}"
                
                page.goto(search_url, wait_until='domcontentloaded', timeout=2000)
                
                if page.query_selector('form[action*="checkcaptcha"]'):
                    browser.close()
                    return None
                
                found_urls = []
                
                organic_selectors = [
                    'a.organic__greenurl',
                    '.serp-item a[href*="http"]',
                    '.organic a[href*="http"]'
                ]
                
                for selector in organic_selectors:
                    links = page.query_selector_all(selector)
                    for link in links[:3]:
                        try:
                            href = link.get_attribute('href')
                            if href:
                                real_url = self.extract_real_url(href)
                                if real_url and self.is_valid_company_site_strict(real_url, company_name):
                                    found_urls.append(real_url)
                        except:
                            continue
                
                unique_urls = list(set(found_urls))
                
                if unique_urls:
                    best_url = self.choose_best_url(unique_urls, company_name)
                    browser.close()
                    return best_url
                
                browser.close()
                return None
                
            except Exception:
                browser.close()
                return None
    
    def _playwright_strategy_fast_headless(self, company_name):
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,  # –ò–ó–ú–ï–ù–ï–ù–û: –±—ã–ª–æ True, –æ—Å—Ç–∞–ª–æ—Å—å True
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled'
                ]
            )
            
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            page = context.new_page()
            
            try:
                clean_company = re.sub(r'[\(\)\[\]\{\}]', '', company_name)
                clean_company = re.sub(r'–ò–ü\s+\w+\s+\w+', '', clean_company).strip()
                
                search_query = f"{clean_company} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
                search_url = f"https://yandex.ru/search/?text={quote_plus(search_query)}"
                
                page.goto(search_url, wait_until='domcontentloaded', timeout=20000)
                
                found_urls = []
                
                links = page.query_selector_all('a[href*="http"]')
                for link in links[:8]:
                    try:
                        href = link.get_attribute('href')
                        if href and not href.startswith('https://yandex.ru'):
                            real_url = self.extract_real_url(href)
                            if real_url and self.is_valid_company_site_strict(real_url, company_name):
                                found_urls.append(real_url)
                    except:
                        continue
                
                unique_urls = list(set(found_urls))
                
                if unique_urls:
                    best_url = self.choose_best_url(unique_urls, company_name)
                    browser.close()
                    return best_url
                
                browser.close()
                return None
                
            except Exception:
                browser.close()
                return None
    
    def extract_real_url(self, url):
        if 'yandex.ru/redir/' in url or 'clck' in url:
            try:
                return url
            except Exception:
                pass
        
        return url
    
    def is_relevant_link(self, link_text, company_name):
        if not link_text:
            return False
        
        company_words = company_name.lower().split()
        clean_company = re.sub(r'[\(\)\[\]\{\}]', '', company_name).lower()
        
        ignore_words = [
            '—è–Ω–¥–µ–∫—Å', 'yandex', 'google', '–∫–∞—Ä—Ç—ã', 'images', '–≤–∏–¥–µ–æ', '–Ω–æ–≤–æ—Å—Ç–∏',
            '–º–∞—Ä–∫–µ—Ç', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏', '–Ω–∞–π—Ç–∏', 'search', '–µ—â—ë', 'more', '–µ—â–µ',
            '–∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è', '–ø–æ–≥–æ–¥–∞', '–∞—Ñ–∏—à–∞', '—Ä–∞–±–æ—Ç–∞', '–≤–∞–∫–∞–Ω—Å–∏–∏',
            '—Ä–µ–∑—é–º–µ', 'hh.ru', 'headhunter', '–æ—Ç–∑—ã–≤—ã', '—Ä–µ–π—Ç–∏–Ω–≥', '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞'
        ]
        
        if any(word in link_text.lower() for word in ignore_words):
            return False
        
        for word in company_words:
            if len(word) > 2 and word in link_text.lower():
                return True
        
        return False
    
    def choose_best_url(self, urls, company_name):
        if not urls:
            return None
        
        company_keywords = company_name.lower().split()
        best_score = 0
        best_url = urls[0]
        
        for url in urls:
            score = 0
            domain = urlparse(url).netloc.lower()
            
            for keyword in company_keywords:
                if len(keyword) > 2 and keyword in domain:
                    score += 3
            
            if domain.endswith('.ru'):
                score += 1
            if 'www.' in domain:
                score += 1
            
            if score > best_score:
                best_score = score
                best_url = url
        
        return best_url
    
    def is_valid_company_site(self, url):
        try:
            domain = urlparse(url).netloc.lower()
            
            blacklist = [
                'yandex.ru', 'yandex.com', 'ya.ru', 'google.com', 'google.ru',
                'vk.com', 'facebook.com', 'instagram.com', 'hh.ru', 'rabota.ru', 
                'superjob.ru', 'mail.ru', 'rambler.ru', '2gis.ru', 'gosuslugi.ru'
            ]
            
            if domain in blacklist:
                return False
                
            if any(bad in domain for bad in ['yandex.', 'google.']):
                return False
            
            return True
            
        except:
            return False
    
    def is_valid_company_site_strict(self, url, company_name=None):
        try:
            domain = urlparse(url).netloc.lower()
            
            blacklist = [
                'yandex.ru', 'yandex.com', 'ya.ru', 'google.com', 'google.ru',
                'vk.com', 'facebook.com', 'instagram.com', 'hh.ru', 'rabota.ru', 
                'superjob.ru', 'mail.ru', 'rambler.ru', '2gis.ru', 'gosuslugi.ru',
                'yandex.net', 'yastatic.net', 'yandex.st', 'yandex.ua', 'yandex.kz',
                'yandex.by', 'yandex.az', 'yandex.com.tr', 'kinopoisk.ru', 'market.yandex.ru',
                'youtube.com', 'twitter.com', 'linkedin.com', 't.me', 'telegram.me',
                'whatsapp.com', 'viber.com', 'skype.com', 'zoom.us', 'avito.ru',
                'cian.ru', 'irr.ru', 'banki.ru', 'sravni.ru'
            ]
            
            if domain in blacklist:
                return False
                
            if any(bad in domain for bad in [
                'yandex.', 'google.', 'mail.', 'rambler.', 'search.', 'images.',
                'video.', 'maps.', 'news.', 'market.', 'kinopoisk.', 'social.',
                'chat.', 'messenger.', 'app.', 'api.', 'cdn.', 'static.', 'ad.',
                'ads.', 'analytic', 'tracking'
            ]):
                return False
            
            main_domain = domain.split('.')[0]
            if len(main_domain) < 3:
                return False
            
            if company_name:
                company_lower = company_name.lower()
                company_words = [word for word in company_lower.split() if len(word) > 2]
                domain_matches = any(word in domain for word in company_words)
                
                if domain_matches:
                    return True
            
            return True
            
        except:
            return False

class HHParser:
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.api_client = HHApiClient()
        self.website_finder = CompanyWebsiteFinder()
        self.results = []
    
    def search_vacancies_hybrid(self, keywords, area=113, total_vacancies=None, per_keyword=None):
        all_vacancies = []
        
        if per_keyword and per_keyword > 0:
            for keyword in keywords:
                print(f"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}' (–ª–∏–º–∏—Ç: {per_keyword})")
                api_vacancies = self.search_via_api(keyword, area, per_keyword)
                if api_vacancies:
                    all_vacancies.extend(api_vacancies)
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è '{keyword}': {len(api_vacancies)}")
        elif total_vacancies and total_vacancies > 0:
            vacancies_per_keyword = max(1, total_vacancies // len(keywords))
            print(f"üîç –û–±—â–∏–π –ª–∏–º–∏—Ç: {total_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π (~{vacancies_per_keyword} –Ω–∞ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ)")
            
            for keyword in keywords:
                print(f"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}'")
                api_vacancies = self.search_via_api(keyword, area, vacancies_per_keyword)
                if api_vacancies:
                    all_vacancies.extend(api_vacancies)
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è '{keyword}': {len(api_vacancies)}")
        else:
            for keyword in keywords:
                print(f"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}' (–±–µ–∑ –ª–∏–º–∏—Ç–∞)")
                api_vacancies = self.search_via_api(keyword, area, 100)
                if api_vacancies:
                    all_vacancies.extend(api_vacancies)
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è '{keyword}': {len(api_vacancies)}")
        
        unique_vacancies = []
        seen_urls = set()
        for vacancy in all_vacancies:
            url = vacancy.get('–°—Å—ã–ª–∫–∞_–Ω–∞_–≤–∞–∫–∞–Ω—Å–∏—é', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_vacancies.append(vacancy)
        
        print(f"üìä –ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(unique_vacancies)}")
        return unique_vacancies
    
    def search_via_api(self, keyword, area=113, per_page=100):
        vacancies_data = []
        page = 0
        
        try:
            while len(vacancies_data) < per_page:
                data = self.api_client.search_vacancies(text=keyword, area=area, page=page, per_page=min(100, per_page - len(vacancies_data)))
                if not data or 'items' not in data or not data['items']:
                    break
                
                items = data['items']
                for vacancy in items:
                    if len(vacancies_data) >= per_page:
                        break
                    vacancy_info = self.process_api_vacancy(vacancy, keyword)
                    if vacancy_info:
                        vacancies_data.append(vacancy_info)
                
                page += 1
                if page >= data.get('pages', 1):
                    break
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
        
        return vacancies_data
    
    def process_api_vacancy(self, vacancy, keyword):
        try:
            title = vacancy.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            company_info = vacancy.get('employer', {})
            company_name = company_info.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            vacancy_url = vacancy.get('alternate_url', '')
            
            company_website = self.website_finder.find_company_website(company_name)
            
            vacancy_data = {
                '–ù–∞–∑–≤–∞–Ω–∏–µ_–≤–∞–∫–∞–Ω—Å–∏–∏': title,
                '–ö–ª—é—á–µ–≤–æ–µ_—Å–ª–æ–≤–æ': keyword,
                '–ö–æ–º–ø–∞–Ω–∏—è': company_name,
                '–°—Å—ã–ª–∫–∞_–Ω–∞_–≤–∞–∫–∞–Ω—Å–∏—é': vacancy_url,
                '–°–∞–π—Ç_–∫–æ–º–ø–∞–Ω–∏–∏': company_website if company_website else '–ù–µ –Ω–∞–π–¥–µ–Ω',
                '–ì–æ—Ä–æ–¥': self.extract_area(vacancy),
            }
            
            return vacancy_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
            return None
    
    def extract_area(self, vacancy):
        area_info = vacancy.get('area', {})
        return area_info.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω')
    
    def generate_filename(self, keywords):
        main_keyword = keywords[0] if keywords else "vacancies"
        clean_keyword = re.sub(r'[<>:"/\\|?*]', '', main_keyword).replace(' ', '_')[:50]
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        return f"{clean_keyword}_{current_time}.xlsx"
    
    def save_to_excel(self, keywords, custom_filename=None):
        if not self.results:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return False, None
        
        try:
            df = pd.DataFrame(self.results)
            
            if not os.path.exists('results'):
                os.makedirs('results')
            
            filename = custom_filename if custom_filename else self.generate_filename(keywords)
            filepath = os.path.join('results', filename)
            
            column_mapping = {
                '–ù–∞–∑–≤–∞–Ω–∏–µ_–≤–∞–∫–∞–Ω—Å–∏–∏': '–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏',
                '–ö–ª—é—á–µ–≤–æ–µ_—Å–ª–æ–≤–æ': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ', 
                '–ö–æ–º–ø–∞–Ω–∏—è': '–ö–æ–º–ø–∞–Ω–∏—è',
                '–°—Å—ã–ª–∫–∞_–Ω–∞_–≤–∞–∫–∞–Ω—Å–∏—é': '–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é',
                '–°–∞–π—Ç_–∫–æ–º–ø–∞–Ω–∏–∏': '–°–∞–π—Ç –∫–æ–º–ø–∞–Ω–∏–∏',
                '–ì–æ—Ä–æ–¥': '–ì–æ—Ä–æ–¥'
            }
            
            df = df.rename(columns=column_mapping)
            
            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='–í–∞–∫–∞–Ω—Å–∏–∏')
                
                workbook = writer.book
                worksheet = writer.sheets['–í–∞–∫–∞–Ω—Å–∏–∏']
                
                from openpyxl.styles import Font
                link_font = Font(color="0563C1", underline="single")
                
                for row in range(2, len(df) + 2):
                    cell = worksheet[f'D{row}']
                    if cell.value and str(cell.value).startswith('http'):
                        cell.hyperlink = cell.value
                        cell.font = link_font
                    
                    cell = worksheet[f'E{row}']
                    if cell.value and str(cell.value).startswith('http'):
                        cell.hyperlink = cell.value
                        cell.font = link_font
            
            full_path = os.path.abspath(filepath)
            print(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {full_path}")
            print(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
            
            sites_found = sum(1 for r in self.results if r.get('–°–∞–π—Ç_–∫–æ–º–ø–∞–Ω–∏–∏') not in [None, '–ù–µ –Ω–∞–π–¥–µ–Ω'])
            print(f"üåê –ù–∞–π–¥–µ–Ω–æ —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π: {sites_found}")
            
            return True, full_path
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            return False, None
    
    def run_parser(self, keywords, area=113, total_vacancies=None, per_keyword=None):
        print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ï–†–ê HH.RU")
        print("=" * 50)
        
        self.results = self.search_vacancies_hybrid(keywords, area, total_vacancies, per_keyword)
        
        if self.results:
            success, filepath = self.save_to_excel(keywords)
            
            if success:
                print("‚úÖ –ü–ê–†–°–ò–ù–ì –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
                print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(self.results)}")
                
                keyword_stats = {}
                for result in self.results:
                    keyword = result.get('–ö–ª—é—á–µ–≤–æ–µ_—Å–ª–æ–≤–æ', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1
                
                print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º:")
                for keyword, count in keyword_stats.items():
                    print(f"   '{keyword}': {count} –≤–∞–∫–∞–Ω—Å–∏–π")
                
            return len(self.results), success, filepath
        else:
            print("‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return 0, False, None

def get_region_id(region_name, api_client):
    if not region_name:
        return "113"
    
    regions = api_client.load_regions()
    if not regions:
        return "113"
    
    clean_name = region_name.lower().strip()
    
    if clean_name in regions:
        return regions[clean_name]
    
    for region_key in regions:
        if clean_name in region_key or region_key in clean_name:
            return regions[region_key]
    
    return "113"

class HHParserGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("HH.ru Parser")
        self.root.minsize(700, 600)
        self.center_window()
        
        self.parser = HHParser()
        self.setup_ui()
        self.load_regions_on_start()
    
    def center_window(self):
        self.root.update_idletasks()
        width = self.root.winfo_reqwidth()
        height = self.root.winfo_reqheight()
        x = (self.root.winfo_screenwidth() // 2) - (width // 2)
        y = (self.root.winfo_screenheight() // 2) - (height // 2)
        self.root.geometry(f"{width}x{height}+{x}+{y}")
    
    def load_regions_on_start(self):
        self.progress_var.set("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–≥–∏–æ–Ω—ã...")
        threading.Thread(target=self._load_regions_thread, daemon=True).start()
    
    def _load_regions_thread(self):
        try:
            regions = self.parser.api_client.load_regions()
            self.root.after(0, lambda: self.regions_loaded(regions))
        except Exception as e:
            self.root.after(0, lambda: self.regions_load_failed())
    
    def regions_loaded(self, regions):
        if regions:
            self.progress_var.set(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(regions)} —Ä–µ–≥–∏–æ–Ω–æ–≤")
    
    def regions_load_failed(self):
        self.progress_var.set("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤")
    
    def setup_ui(self):
        main_frame = ttk.Frame(self.root, padding="15")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(9, weight=1)
        
        title_label = ttk.Label(main_frame, text="–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ HH.ru", font=('Arial', 14, 'bold'))
        title_label.grid(row=0, column=0, columnspan=2, pady=(0, 15))
        
        ttk.Label(main_frame, text="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞:", font=('Arial', 10)).grid(row=1, column=0, sticky=tk.W, pady=5)
        
        self.keywords_text = tk.Text(main_frame, height=5, width=70)
        self.keywords_text.grid(row=2, column=0, columnspan=2, pady=5, sticky=(tk.W, tk.E))
        self.keywords_text.insert(tk.END, "python\n–º–µ–Ω–µ–¥–∂–µ—Ä\n–∞–Ω–∞–ª–∏—Ç–∏–∫\n—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫")
        
        settings_frame = ttk.LabelFrame(main_frame, text="–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
        settings_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        settings_frame.columnconfigure(1, weight=1)
        
        ttk.Label(settings_frame, text="–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–≥–∏–æ–Ω:", font=('Arial', 10, 'bold')).grid(row=0, column=0, columnspan=2, sticky=tk.W, padx=5, pady=8)
        
        self.region_buttons_frame = ttk.Frame(settings_frame)
        self.region_buttons_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), padx=5, pady=5)
        self.region_buttons_frame.columnconfigure(0, weight=1)
        self.region_buttons_frame.columnconfigure(1, weight=1)
        self.region_buttons_frame.columnconfigure(2, weight=1)
        
        self.region_var = tk.StringVar(value="113")
        
        self.russia_rb = ttk.Radiobutton(self.region_buttons_frame, text="–í—Å—è –†–æ—Å—Å–∏—è", variable=self.region_var, value="113")
        self.russia_rb.grid(row=0, column=0, sticky=tk.W, padx=(0, 10))
        
        self.moscow_rb = ttk.Radiobutton(self.region_buttons_frame, text="–ú–æ—Å–∫–≤–∞", variable=self.region_var, value="1")
        self.moscow_rb.grid(row=0, column=1, sticky=tk.W, padx=(0, 10))
        
        self.spb_rb = ttk.Radiobutton(self.region_buttons_frame, text="–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", variable=self.region_var, value="2")
        self.spb_rb.grid(row=0, column=2, sticky=tk.W)
        
        ttk.Label(settings_frame, text="–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ –≥–æ—Ä–æ–¥ –≤—Ä—É—á–Ω—É—é:").grid(row=2, column=0, sticky=tk.W, padx=5, pady=8)
        self.custom_region_var = tk.StringVar()
        self.custom_region_entry = ttk.Entry(settings_frame, textvariable=self.custom_region_var, width=30)
        self.custom_region_entry.grid(row=2, column=1, sticky=tk.W, padx=5, pady=8)
        
        if hasattr(self.custom_region_var, 'trace_add'):
            self.custom_region_var.trace_add('write', self.on_custom_region_change)
        else:
            self.custom_region_var.trace('w', self.on_custom_region_change)
        
        limits_frame = ttk.LabelFrame(main_frame, text="–õ–∏–º–∏—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π")
        limits_frame.grid(row=4, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        limits_frame.columnconfigure(1, weight=1)
        
        self.limit_mode = tk.StringVar(value="none")
        
        ttk.Radiobutton(limits_frame, text="–ë–µ–∑ –ª–∏–º–∏—Ç–∞", variable=self.limit_mode, value="none").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
        
        ttk.Radiobutton(limits_frame, text="–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π:", variable=self.limit_mode, value="total").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)
        self.total_vacancies_var = tk.StringVar(value="100")
        self.total_vacancies_entry = ttk.Entry(limits_frame, textvariable=self.total_vacancies_var, width=10)
        self.total_vacancies_entry.grid(row=1, column=1, sticky=tk.W, padx=5, pady=5)
        
        ttk.Radiobutton(limits_frame, text="–ù–∞ –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ:", variable=self.limit_mode, value="per_keyword").grid(row=2, column=0, sticky=tk.W, padx=5, pady=5)
        self.per_keyword_var = tk.StringVar(value="50")
        self.per_keyword_entry = ttk.Entry(limits_frame, textvariable=self.per_keyword_var, width=10)
        self.per_keyword_entry.grid(row=2, column=1, sticky=tk.W, padx=5, pady=5)
        
        stats_frame = ttk.LabelFrame(main_frame, text="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞")
        stats_frame.grid(row=5, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        
        self.stats_var = tk.StringVar(value="–í–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ: 0")
        stats_label = ttk.Label(stats_frame, textvariable=self.stats_var, font=('Arial', 11, 'bold'))
        stats_label.grid(row=0, column=0, sticky=tk.W, padx=10, pady=8)
        
        self.progress_var = tk.StringVar(value="–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤...")
        progress_label = ttk.Label(stats_frame, textvariable=self.progress_var, font=('Arial', 9))
        progress_label.grid(row=1, column=0, sticky=tk.W, padx=10, pady=4)
        
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=6, column=0, columnspan=2, pady=15)
        
        self.parse_btn = ttk.Button(button_frame, text="–ù–∞—á–∞—Ç—å –ø–æ–∏—Å–∫", command=self.start_parsing, width=15)
        self.parse_btn.pack(side=tk.LEFT, padx=5)
        
        self.export_btn = ttk.Button(button_frame, text="–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel", command=self.export_to_excel, width=15)
        self.export_btn.pack(side=tk.LEFT, padx=5)
        
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=7, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
    
    def on_custom_region_change(self, *args):
        custom_text = self.custom_region_var.get().strip()
        if custom_text:
            self.region_buttons_frame.grid_remove()
        else:
            self.region_buttons_frame.grid()
    
    def get_selected_region(self):
        if self.custom_region_var.get().strip():
            region_name = self.custom_region_var.get().strip()
            return get_region_id(region_name, self.parser.api_client)
        else:
            return self.region_var.get()
    
    def get_vacancies_limits(self):
        limit_mode = self.limit_mode.get()
        
        if limit_mode == "total":
            try:
                total = int(self.total_vacancies_var.get())
                return total, None
            except:
                return None, None
        elif limit_mode == "per_keyword":
            try:
                per_keyword = int(self.per_keyword_var.get())
                return None, per_keyword
            except:
                return None, None
        else:
            return None, None
    
    def start_parsing(self):
        keywords_text = self.keywords_text.get(1.0, tk.END).strip()
        if not keywords_text:
            messagebox.showerror("–û—à–∏–±–∫–∞", "–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
            return
        
        keywords = [k.strip() for k in keywords_text.split('\n') if k.strip()]
        area = self.get_selected_region()
        total_vacancies, per_keyword = self.get_vacancies_limits()
        
        region_name = "–í—Å—è –†–æ—Å—Å–∏—è"
        if area == "1":
            region_name = "–ú–æ—Å–∫–≤–∞"
        elif area == "2":
            region_name = "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"
        elif self.custom_region_var.get().strip():
            region_name = self.custom_region_var.get().strip()
        
        limit_info = ""
        if total_vacancies:
            limit_info = f" (–æ–±—â–∏–π –ª–∏–º–∏—Ç: {total_vacancies})"
        elif per_keyword:
            limit_info = f" (–Ω–∞ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {per_keyword})"
        
        self.stats_var.set("–ò–¥–µ—Ç –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π...")
        self.progress_var.set(f"–†–µ–≥–∏–æ–Ω: {region_name}{limit_info}")
        
        thread = threading.Thread(target=self.run_parser, args=(keywords, area, total_vacancies, per_keyword))
        thread.daemon = True
        thread.start()
        
        self.parse_btn.config(state='disabled')
        self.progress.start()
    
    def run_parser(self, keywords, area, total_vacancies, per_keyword):
        try:
            vacancies_count, success, filepath = self.parser.run_parser(keywords, int(area), total_vacancies, per_keyword)
            self.root.after(0, lambda: self.parsing_completed(vacancies_count, success, filepath))
        except Exception as e:
            self.root.after(0, lambda: self.parsing_failed())
    
    def parsing_completed(self, vacancies_count, success, filepath):
        self.progress.stop()
        self.parse_btn.config(state='normal')
        
        self.stats_var.set(f"–í–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ: {vacancies_count}")
        
        if success:
            self.progress_var.set("–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            
            if vacancies_count > 0:
                messagebox.showinfo("–£—Å–ø–µ—Ö", f"–ù–∞–π–¥–µ–Ω–æ {vacancies_count} –≤–∞–∫–∞–Ω—Å–∏–π\n–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {os.path.basename(filepath)}")
            else:
                messagebox.showinfo("–†–µ–∑—É–ª—å—Ç–∞—Ç", "–í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            self.progress_var.set("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏")
            messagebox.showerror("–û—à–∏–±–∫–∞", "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    def parsing_failed(self):
        self.progress.stop()
        self.parse_btn.config(state='normal')
        self.stats_var.set("–í–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ: 0")
        self.progress_var.set("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ")
        messagebox.showerror("–û—à–∏–±–∫–∞", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ")
    
    def export_to_excel(self):
        if not self.parser.results:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return
        
        keywords_text = self.keywords_text.get(1.0, tk.END).strip()
        keywords = [k.strip() for k in keywords_text.split('\n') if k.strip()]
        default_filename = self.parser.generate_filename(keywords)
        
        filename = filedialog.asksaveasfilename(
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")],
            initialfile=default_filename
        )
        
        if filename:
            success, filepath = self.parser.save_to_excel(keywords, filename)
            if success:
                messagebox.showinfo("–£—Å–ø–µ—Ö", f"–î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤:\n{filepath}")

if __name__ == "__main__":
    root = tk.Tk()
    app = HHParserGUI(root)
    
    try:
        root.mainloop()
    finally:
        pass